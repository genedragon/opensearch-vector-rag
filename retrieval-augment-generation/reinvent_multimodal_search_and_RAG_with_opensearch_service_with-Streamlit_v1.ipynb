{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082aa845",
   "metadata": {},
   "source": [
    "# Build Multimodal RAG with Amazon OpenSearch Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7b731",
   "metadata": {},
   "source": [
    "In this notebook, you will build and run multimodal search using a sample retail dataset. You will use multimodal generated embeddings for text and image and experiment by running text search only, image search only and both text and image search in OpenSearch Service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f69f6",
   "metadata": {},
   "source": [
    "You will be using a retail dataset that contains 2,465 retail product samples that belong to different categories such as accessories, home decor, apparel, housewares, books, and instruments. Each product contains metadata including the ID, current stock, name, category, style, description, price, image URL, and gender affinity of the product. You will be using only the product image and product description fields in the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ae065",
   "metadata": {},
   "source": [
    " \n",
    "Step 1: Create embeddings for text and images\n",
    "\n",
    "Step 2: Store the embeddings in OpenSearch Service index\n",
    "\n",
    "Step 3: Use LLM to generate text using the context from OpenSearch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Step 1: \n",
    "\n",
    "1. Build AI/connector between AOS and Embedding model - Titan Mulitmodal embeddings model\n",
    "2. Register/Deploy the Embedding model in AOS\n",
    "3. Create a KNN index in AOS\n",
    "4. Create an ingest pipeline to generate the embedding inside AOS\n",
    "\n",
    "Step 2:\n",
    "\n",
    "1. Index the data\n",
    "\n",
    "Step 3:\n",
    "\n",
    "1. Run multimodal neural search query in AOS \n",
    "2. Feed the LLM with the extract results from AOS - Claude Sonnet 3 \n",
    "2.1. Build AI/connector between AOS and LLM to generate the text\n",
    "2.2. Register/Deploy the LLM in AOS\n",
    "2.3. Run conversational search query in AOS (using the LLM model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be245f",
   "metadata": {},
   "source": [
    "## 1. Lab Pre-requisites\n",
    "\n",
    "\n",
    "For this notebook we require a few libraries. We'll use the Python clients for Amazon OpenSearch Service and Amazon Bedrock, and OpenSearch ML Client library for generating multimodal embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df92a75",
   "metadata": {},
   "source": [
    "#### 1.1. Import libraries & initialize resource information\n",
    "The line below will import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d38375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n",
    "import random \n",
    "import string\n",
    "import s3fs\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240112b",
   "metadata": {},
   "source": [
    "#### 1.2. Get CloudFormation stack output variables\n",
    "\n",
    "We have preconfigured a few resources by creating a CloudFormation stack in the account. Names and ARN of these resources will be used within this lab. We are going to load some of the information variables here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"advaned-rag-opensearch\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b636855",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1eae6",
   "metadata": {},
   "source": [
    "### 2.1.Download the dataset (.gz) and extract the .gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "os.makedirs('tmp/images', exist_ok = True)\n",
    "metadata_file = urllib.request.urlretrieve('https://aws-blogs-artifacts-public.s3.amazonaws.com/BDB-3144/products-data.yml', 'tmp/images/products.yaml')\n",
    "img_filename,headers= urllib.request.urlretrieve('https://aws-blogs-artifacts-public.s3.amazonaws.com/BDB-3144/images.tar.gz', 'tmp/images/images.tar.gz')              \n",
    "print(img_filename)\n",
    "file = tarfile.open('tmp/images/images.tar.gz')\n",
    "file.extractall('tmp/images/')\n",
    "file.close()\n",
    "#remove images.tar.gz\n",
    "os.remove('tmp/images/images.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f307e7",
   "metadata": {},
   "source": [
    "## 3. Create a connection with OpenSearch domain.\n",
    "Next, we'll use Python API to set up connection with OpenSearch domain.\n",
    "\n",
    "#### Important pre-requisite\n",
    "You should have followed the steps in the Lab instruction section to map Sagemaker notebook role to OpenSearch `ml_full_access` role. If not, please visit the lab instructions and complete the **Setting up permission for Notebook IAM Role** section.\n",
    "\n",
    "#### Retrieving credentials from Secrets manager\n",
    "We are going to use Amazon Sagemaker Notebook IAM role to configure the workflows in OpenSearch. This IAM Role has permission to pass BedrockInference IAM role to OpenSearch. OpenSearch will then be able to use BedrockInference IAM role to make calls to Bedrock models.\n",
    "\n",
    "##### NOTE: \n",
    "_At any point in this lab, if you get a failure message - **The security token included in the request is expired.**_ You can resolve it by running this cell again. The cell refreshes the security credentials that is required for the rest of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21309d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['OpenSearchSecret'])['SecretString'])\n",
    "\n",
    "#credentials = boto3.Session().get_credentials()\n",
    "#auth = AWSV4SignerAuth(credentials, region)\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "ml_client = MLCommonClient(aos_client)\n",
    "\n",
    "#initializing some variables that we will use later.\n",
    "\n",
    "embedding_connector_id = \"\"\n",
    "embedding_model_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d0400",
   "metadata": {},
   "source": [
    "## 4. Create and deploy model connector to Amazon Bedrock Titan Multimodal Embedding\n",
    "\n",
    "Following cell will create a connector using SageMaker Notebook IAM role. Following cell will create a OpenSearch remote model connector with Amazon Bedrock Titan MM Embedding model. Following cell defines the connector configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f468131",
   "metadata": {},
   "source": [
    "## 3. Create the OpenSearch Bedrock ML connector\n",
    "\n",
    "you need to change **\"iam-role-arn\"** below with the ARN of the IAM role that has permissions to talk to OpenSearch and mapped as back-end role in OpenSearch dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a113cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests \n",
    "from requests_aws4auth import AWS4Auth\n",
    "import json\n",
    "\n",
    "\n",
    "if not embedding_connector_id:\n",
    "    host = f'https://{aos_host}/'\n",
    "    service = 'es'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "\n",
    "    # Register repository\n",
    "    path = '_plugins/_ml/connectors/_create'\n",
    "    url = host + path\n",
    "\n",
    "    payload = {\n",
    "        \"name\": \"Amazon Bedrock Connector: embedding\",\n",
    "        \"description\": \"The connector to bedrock Titan multimodal embedding model\",\n",
    "        \"version\": 1,\n",
    "        \"protocol\": \"aws_sigv4\",\n",
    "        \"credential\": {\n",
    "          \"roleArn\": f\"arn:aws:iam::{account_id}:role/{bedrock_inf_iam_role}\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "        \"region\": region,\n",
    "        \"service_name\": \"bedrock\",\n",
    "        \"model\": \"amazon.titan-embed-image-v1\"\n",
    "       },\n",
    "       \"actions\": [\n",
    "        {\n",
    "          \"action_type\": \"predict\",\n",
    "          \"method\": \"POST\",\n",
    "          \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "          \"headers\": {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"x-amz-content-sha256\": \"required\"\n",
    "          },\n",
    "         \"request_body\": \"{ \\\"inputText\\\": \\\"${parameters.inputText}\\\" }\",\n",
    "         \"pre_process_function\": \"connector.pre_process.bedrock.embedding\",\n",
    "         \"post_process_function\": \"connector.post_process.bedrock.embedding\"}\n",
    "       ]\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    print(r.status_code)\n",
    "    print(r.text)\n",
    "    embedding_connector_id = json.loads(r.text)[\"connector_id\"]\n",
    "else:\n",
    "    print(f\"Connector already exists - {embedding_connector_id}\")\n",
    "    \n",
    "embedding_connector_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eed3ef",
   "metadata": {},
   "source": [
    "Once the model connector is defined. We need to register the model and deploy. Following two cells will register and then deploy the model connection respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbe781",
   "metadata": {},
   "source": [
    "Once the model connector is defined. We need to register the model and deploy. Following two cells will register and then deploy the model connection respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the embedding model\n",
    "if not embedding_model_id:\n",
    "    path = '_plugins/_ml/models/_register'\n",
    "    url = 'https://'+aos_host + '/' + path\n",
    "    payload = { \"name\": \"Bedrock Titan mm embeddings model\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"Bedrock Titan mm embeddings model\",\n",
    "    \"connector_id\": embedding_connector_id}\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    embedding_model_id = json.loads(r.text)[\"model_id\"]\n",
    "else:\n",
    "    print(\"skipping model registration - model already exists\")\n",
    "print(\"Model registered under model_id: \"+embedding_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387db013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the embedding model\n",
    "path = '_plugins/_ml/models/'+embedding_model_id+'/_deploy'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, headers=headers)\n",
    "deploy_status = json.loads(r.text)[\"status\"]\n",
    "print(\"Deployment status of the model, \"+embedding_model_id+\" : \"+deploy_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852500c8",
   "metadata": {},
   "source": [
    "## 4. Test the OpenSearch - Bedrock integration with a test input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6153286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "\n",
    "\n",
    "path = '_plugins/_ml/models/'+embedding_model_id+'/_predict'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "img = \"tmp/images/footwear/2d2d8ec8-4806-42a7-b8ba-ceb15c1c7e84.jpg\"\n",
    "with open(img, \"rb\") as image_file:\n",
    "    input_image_binary = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "    \n",
    "payload = {\n",
    "\"parameters\": {\n",
    "\"inputText\": \"Sleek, stylish black sneakers made for urban exploration. With fashionable looks and comfortable design, these sneakers keep your feet looking great while you walk the city streets in style\",\n",
    "\"inputImage\":input_image_binary\n",
    "}\n",
    "}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "\n",
    "try:\n",
    "    embed = json.loads(r.text)['inference_results'][0]['output'][0]['data'][0:10]\n",
    "    shape = json.loads(r.text)['inference_results'][0]['output'][0]['shape'][0]\n",
    "    print(\"First 10 dimensions:\")\n",
    "    print(str(embed))\n",
    "    print(\"\\n\")\n",
    "    print(\"Total: \" + str(shape) + \" dimensions\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")\n",
    "    print(\"The response does not contain the expected data structure.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"An unexpected error occurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e12bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd7e48",
   "metadata": {},
   "source": [
    "## 5. Create the OpenSearch ingest pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d48551",
   "metadata": {},
   "source": [
    "## 5. Create ingest pipeline\n",
    "Let's create an ingestion pipeline that will call Amazon Bedrock Titan Multimodal embedding model and convert the text and image into multimodal vector embedding. Ingest pipeline is a feature in OpenSearch that allows you to define certain actions to be performed at the time of data ingestion. You could do simple processing such as adding a static field, modify an existing field, or call a remote model to get inference and store inference output together with the indexed record/document. In our case inference output is vector embedding.\n",
    "\n",
    "Following ingestion pipeline is going to call our remote model and convert product image `product_description` field and the `image_binary` to vector and store it in the field called `vector_embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"_ingest/pipeline/bedrock-multimodal-ingest-pipeline\"\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "payload = {\n",
    "\"description\": \"A text/image embedding pipeline\",\n",
    "\"processors\": [\n",
    "{\n",
    "\"text_image_embedding\": {\n",
    "\"model_id\":embedding_model_id,\n",
    "\"embedding\": \"vector_embedding\",\n",
    "\"field_map\": {\n",
    "\"text\": \"product_description\",\n",
    "\"image\": \"image_binary\"\n",
    "}}}]}\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f358ebf",
   "metadata": {},
   "source": [
    "## 6. Create the k-NN index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"bedrock-multimodal-rag\"\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "\n",
    "#this will delete the index if already exists\n",
    "requests.delete(url, auth=awsauth, json=payload, headers=headers)\n",
    "\n",
    "payload = {\n",
    "  \"settings\": {\n",
    "    \"index.knn\": True,\n",
    "    \"default_pipeline\": \"bedrock-multimodal-ingest-pipeline\"\n",
    "  },\n",
    "  \"mappings\": {\n",
    "      \n",
    "    \"_source\": {\n",
    "     \n",
    "      \"excludes\": [\"image_binary\"]\n",
    "    },\n",
    "    \"properties\": {\n",
    "      \"vector_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": shape,\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"faiss\",\n",
    "          \"parameters\": {}\n",
    "        }\n",
    "      },\n",
    "      \"product_description\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "        \"image_url\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"image_binary\": {\n",
    "        \"type\": \"binary\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42df6cf",
   "metadata": {},
   "source": [
    "## 7. Ingest the dataset into k-NN index usig Bulk request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea00f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "from PIL import Image\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "def resize_image(photo, width, height):\n",
    "    Image.MAX_IMAGE_PIXELS = 100000000\n",
    "    \n",
    "    with Image.open(photo) as image:\n",
    "        image.verify()\n",
    "    with Image.open(photo) as image:    \n",
    "        \n",
    "        if image.format in [\"JPEG\", \"PNG\"]:\n",
    "            file_type = image.format.lower()\n",
    "            path = image.filename.rsplit(\".\", 1)[0]\n",
    "\n",
    "            image.thumbnail((width, height))\n",
    "            image.save(f\"{path}-resized.{file_type}\")\n",
    "    return file_type, path\n",
    "\n",
    "# Load the products from the dataset\n",
    "yaml = YAML()\n",
    "items_ = yaml.load(open('tmp/images/products.yaml'))\n",
    "\n",
    "batch = 0\n",
    "count = 0\n",
    "body_ = ''\n",
    "batch_size = 100\n",
    "last_batch = int(len(items_)/batch_size)\n",
    "action = json.dumps({ 'index': { '_index': 'bedrock-multimodal-rag' } })\n",
    "\n",
    "for item in items_:\n",
    "    count+=1\n",
    "    fileshort = \"tmp/images/\"+item[\"category\"]+\"/\"+item[\"image\"]\n",
    "    payload = {}\n",
    "    payload['image_url'] = fileshort\n",
    "    payload['product_description'] = item['description']\n",
    "    \n",
    "    #resize the image and generate image binary\n",
    "    file_type, path = resize_image(fileshort, 2048, 2048)\n",
    "\n",
    "    with open(fileshort.split(\".\")[0]+\"-resized.\"+file_type, \"rb\") as image_file:\n",
    "        input_image = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "    \n",
    "    os.remove(fileshort.split(\".\")[0]+\"-resized.\"+file_type)\n",
    "    payload['image_binary'] = input_image\n",
    "    \n",
    "    body_ = body_ + action + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "    \n",
    "    if(count == batch_size):\n",
    "        response = aos_client.bulk(\n",
    "        index = 'bedrock-multimodal-rag',\n",
    "        body = body_\n",
    "        )\n",
    "        batch += 1\n",
    "        count = 0\n",
    "        print(\"batch \"+str(batch) + \" ingestion done!\")\n",
    "        if(batch != last_batch):\n",
    "            body_ = \"\"\n",
    "        \n",
    "            \n",
    "#ingest the remaining rows\n",
    "response = aos_client.bulk(\n",
    "        index = 'bedrock-multimodal-rag',\n",
    "        body = body_\n",
    "        )\n",
    "        \n",
    "print(\"All \"+str(last_batch)+\" batches ingested into index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b08140",
   "metadata": {},
   "source": [
    "## 8. Experiment 1: Keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbb176",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Keyword Search\n",
    "query = \"trendy footwear for women\"\n",
    "url = 'https://' + aos_host + \"/bedrock-multimodal-rag/_search\"\n",
    "keyword_payload = {\"_source\": {\n",
    "        \"exclude\": [\n",
    "            \"vector_embedding\"\n",
    "        ]\n",
    "        },\n",
    "        \"query\": {    \"match\": {\n",
    "                        \"product_description\": {\n",
    "                            \"query\": query\n",
    "                        }\n",
    "                        }\n",
    "                    }\n",
    "        \n",
    "        ,\"size\":5,\n",
    "  }\n",
    "\n",
    "r = requests.get(url, auth=awsauth, json=keyword_payload, headers=headers)\n",
    "response_ = json.loads(r.text)\n",
    "docs = response_['hits']['hits']\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(str(i+1)+ \". \"+doc[\"_source\"][\"product_description\"])\n",
    "    image = Image.open(doc[\"_source\"][\"image_url\"])\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34691181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "809face1",
   "metadata": {},
   "source": [
    "## 9. Experiment 2: Multimodal search with only text caption as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db1381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Multimodal Search\n",
    "#Text as input\n",
    "\n",
    "query = \"trendy footwear for women\"\n",
    "url = 'https://'+aos_host+\"/bedrock-multimodal-rag/_search\"\n",
    "text_embedding_payload = {\"_source\": {\n",
    "        \"exclude\": [\n",
    "            \"vector_embedding\"\n",
    "        ]\n",
    "        },\n",
    "        \"query\": {    \n",
    "       \n",
    "        \"neural\": {\n",
    "            \"vector_embedding\": {\n",
    "                \n",
    "            #\"query_image\":query_image_binary,\n",
    "            \"query_text\":query,\n",
    "                \n",
    "            \"model_id\": embedding_model_id,\n",
    "            \"k\": 3\n",
    "            }\n",
    "            }\n",
    "            \n",
    "                    }\n",
    "        \n",
    "        ,\"size\":5,\n",
    "  }\n",
    "\n",
    "r = requests.get(url, auth=awsauth, json=text_embedding_payload, headers=headers)\n",
    "response_ = json.loads(r.text)\n",
    "docs = response_['hits']['hits']\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(doc[\"_source\"][\"product_description\"])\n",
    "    image = Image.open(doc[\"_source\"][\"image_url\"])\n",
    "    image.show()\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ccfd57",
   "metadata": {},
   "source": [
    "## 10. Experiment 3: Multimodal search with only image as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multimodal Search\n",
    "#image as input\n",
    "import urllib.request\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "image_file = urllib.request.urlretrieve('https://aws-blogs-artifacts-public.s3.amazonaws.com/BDB-3144/women_wear.jpg', 'tmp/women-footwear-1.jpg')\n",
    "img = Image.open(\"tmp/women-footwear-1.jpg\") \n",
    "print(\"Input query Image:\")\n",
    "img.show()\n",
    "with open(\"tmp/women-footwear-1.jpg\", \"rb\") as image_file:\n",
    "    query_image_binary = base64.b64encode(image_file.read()).decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(query_image_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://'+aos_host+\"/bedrock-multimodal-rag/_search\"\n",
    "keyword_payload = {\"_source\": {\n",
    "        },\n",
    "        \"query\": {    \n",
    "       \n",
    "        \"neural\": {\n",
    "            \"vector_embedding\": {\n",
    "            \"query_image\":query_image_binary,\n",
    "            \"model_id\": embedding_model_id,\n",
    "            \"k\": 5\n",
    "            }\n",
    "            \n",
    "            }\n",
    "                    }\n",
    "        \n",
    "        ,\"size\":5,\n",
    "  }\n",
    "\n",
    "r = requests.get(url, auth=awsauth, json=keyword_payload, headers=headers)\n",
    "response_ = json.loads(r.text)\n",
    "docs = response_['hits']['hits']\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(doc[\"_source\"][\"product_description\"])\n",
    "    image = Image.open(doc[\"_source\"][\"image_url\"])\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc95752",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37615a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(img_embedding_payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3107971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1cb55",
   "metadata": {},
   "source": [
    "## 11. Experiment 4: Multimodal search with both image and text caption as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce08e5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Multimodal Search\n",
    "#Text and image as inputs\n",
    "import urllib.request\n",
    "s3 = boto3.client('s3')\n",
    "url = 'https://'+aos_host + \"/bedrock-multimodal-rag/_search\"\n",
    "query = \"trendy footwear for women\"\n",
    "print(\"Input text query: \"+query)\n",
    "# urllib.request.urlretrieve( \n",
    "#   'https://cdn.pixabay.com/photo/2014/09/03/20/15/shoes-434918_1280.jpg',\"tmp/women-footwear.jpg\") \n",
    "img = Image.open(\"tmp/women-footwear-1.jpg\") \n",
    "print(\"Input query Image:\")\n",
    "img.show()\n",
    "with open(\"tmp/women-footwear-1.jpg\", \"rb\") as image_file:\n",
    "    query_image_binary = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "keyword_payload = {\"_source\": {\n",
    "        \"exclude\": [\n",
    "            \"vector_embedding\"\n",
    "        ]\n",
    "        },\n",
    "        \"query\": {    \n",
    "       \n",
    "        \"neural\": {\n",
    "            \"vector_embedding\": {\n",
    "                \n",
    "            \"query_image\":query_image_binary,\n",
    "            \"query_text\":query,\n",
    "                \n",
    "            \"model_id\": embedding_model_id,\n",
    "            \"k\": 5\n",
    "            }\n",
    "            \n",
    "            }\n",
    "                    }\n",
    "        \n",
    "        ,\"size\":5,\n",
    "  }\n",
    "\n",
    "r = requests.get(url, auth=awsauth, json=keyword_payload, headers=headers)\n",
    "response_ = json.loads(r.text)\n",
    "docs = response_['hits']['hits']\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(doc[\"_source\"][\"product_description\"])\n",
    "    image = Image.open(doc[\"_source\"][\"image_url\"])\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1a87f",
   "metadata": {},
   "source": [
    "## 12. Create the OpenSearch Bedrock Claude LLM connector\n",
    "\n",
    "you need to change **\"iam-role-arn\"** below with the ARN of the IAM role that has permissions to talk to OpenSearch and mapped as back-end role in OpenSearch dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff711e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests \n",
    "from requests_aws4auth import AWS4Auth\n",
    "import json\n",
    "\n",
    "#initializing variables that we will use later.\n",
    "\n",
    "llm_connector_id = \"\"\n",
    "llm_model_id = \"\"\n",
    "\n",
    "if not llm_connector_id:\n",
    "    host = f'https://{aos_host}/'\n",
    "    service = 'es'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "\n",
    "    # Register repository\n",
    "    path = '_plugins/_ml/connectors/_create'\n",
    "    url = host + path\n",
    "\n",
    "    payload = {\n",
    "        \"name\": \"Amazon Bedrock Connector: Claude 3 sonnet\",\n",
    "        \"description\": \"The connector to bedrock Claude 3\",\n",
    "        \"version\": 1,\n",
    "        \"protocol\": \"aws_sigv4\",\n",
    "        \"credential\": {\n",
    "          \"roleArn\": f\"arn:aws:iam::{account_id}:role/{bedrock_inf_iam_role}\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "        \"region\": region,\n",
    "        \"service_name\": \"bedrock\",\n",
    "        \"model\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"response_filter\": \"$.content[0].text\",\n",
    "        \"max_tokens_to_sample\": \"8000\",\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "       },\n",
    "       \"actions\": [\n",
    "        {\n",
    "          \"action_type\": \"predict\",\n",
    "          \"method\": \"POST\",\n",
    "          \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "          \"headers\": {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"x-amz-content-sha256\": \"required\"\n",
    "          },\n",
    "         \"request_body\": \"{\\\"messages\\\":[{\\\"role\\\":\\\"user\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"${parameters.inputs}\\\"}]}],\\\"anthropic_version\\\":\\\"${parameters.anthropic_version}\\\",\\\"max_tokens\\\":${parameters.max_tokens_to_sample}}\"}\n",
    "       ]\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    print(r.status_code)\n",
    "    print(r.text)\n",
    "    llm_connector_id = json.loads(r.text)[\"connector_id\"]\n",
    "else:\n",
    "    print(f\"Connector already exists - {llm_connector_id}\")\n",
    "    \n",
    "llm_connector_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd39a04",
   "metadata": {},
   "source": [
    "## 13. Register and deploy the OpenSearch Bedrock Claude LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a500638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the llm model\n",
    "if not llm_model_id:\n",
    "    path = '_plugins/_ml/models/_register'\n",
    "    url = 'https://'+aos_host + '/' + path\n",
    "    payload = { \"name\": \"Amazon Bedrock Connector: Claude 3 sonnet\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"The connector to bedrock Claude 3\",\n",
    "    \"connector_id\": llm_connector_id}\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    llm_model_id = json.loads(r.text)[\"model_id\"]\n",
    "else:\n",
    "    print(\"skipping model registration - llm model already exists\")\n",
    "print(\"Model registered under llm_model_id: \"+llm_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the llm model\n",
    "path = '_plugins/_ml/models/'+llm_model_id+'/_deploy'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, headers=headers)\n",
    "deploy_status = json.loads(r.text)[\"status\"]\n",
    "print(\"Deployment status of the model, \"+llm_model_id+\" : \"+deploy_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb824b38",
   "metadata": {},
   "source": [
    "## 14. Test LLM model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abe4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can test the llm text generation here.\n",
    "payload = {\n",
    "    \"parameters\":\n",
    "    { \n",
    "        \"inputs\": \"what are the most popular shoes styles for women?\"\n",
    "    } \n",
    "}\n",
    "\n",
    "path = '_plugins/_ml/models/'+llm_model_id+'/_predict'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, json=payload , headers=headers )\n",
    "\n",
    "\n",
    "#print status of the API call.\n",
    "print(f\"Status: {r.status_code}. Response:{r.text}\")\n",
    "\n",
    "if r.status_code == 200:\n",
    "    llm_gen = json.loads(r.text)['inference_results'][0]['output'][0]\n",
    "    print(str(llm_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing variables that we will use later.\n",
    "\n",
    "agent_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb039c8",
   "metadata": {},
   "source": [
    "## 15. Register and execute an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you’ll use the embedding model and the Claude model to create a flow agent\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"Fashion stylist agent\",\n",
    "  \"type\": \"conversational_flow\",\n",
    "  \"description\": \"this is a demo agent for fashion stylist\",\n",
    "  \"app_type\": \"rag\",\n",
    "     \"memory\": {\n",
    "        \"type\": \"conversation_index\"\n",
    "    },\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"type\": \"VectorDBTool\",\n",
    "        \"name\": \"shopping_knowledge_base\",\n",
    "      \"parameters\": {\n",
    "        \"model_id\": embedding_model_id,\n",
    "        \"index\": \"bedrock-multimodal-rag\",\n",
    "        \"embedding_field\": \"vector_embedding\",\n",
    "        \"source_field\": [\n",
    "            \"product_description\"\n",
    "        ],\n",
    "          \n",
    "        \"input\": \"${parameters.question}\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"MLModelTool\",\n",
    "        \"name\": \"bedrock_claude_v3_sonnet_model\",\n",
    "      \"description\": \"A general tool to answer any question\",\n",
    "      \"parameters\": {\n",
    "        \"model_id\": llm_model_id,\n",
    "        \"prompt\": \"\\n\\nHuman:You are a professional fashion stylist. You will always recommend product based on the given context. If you don't have enough context, you will ask Human to provide more information. If you don't see any related product to recommend, just say we don't have such product. If you don't know the answer, just say you don't know. \\n\\nContext:\\n${parameters.shopping_knowledge_base.output:-}\\n\\n${parameters.chat_history:-}\\n\\nHuman:${parameters.question}\\n\\nAssistant:\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "path = '_plugins/_ml/agents/_register'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, json=payload , headers=headers )\n",
    "\n",
    "agent_id = json.loads(r.text)[\"agent_id\"]\n",
    "\n",
    "#print status of the API call.\n",
    "print(f\"Status: {r.status_code}. Response:{r.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c095ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can inspect the agent by sending a request to the agents endpoint and providing the agent ID:\n",
    "\n",
    "\n",
    "# You can test the llm text generation here.\n",
    "payload = {\n",
    "    \"parameters\":\n",
    "    { #\"inputs\": \"\\n\\nHuman:hello\\n\\nAssistant:\",\n",
    "        \"inputs\": \"what are the most trendy shoes for women?\",\n",
    "        \"question\": \"what are the most trendy shoes for women?\"\n",
    "    } \n",
    "}\n",
    "\n",
    "path = '_plugins/_ml/agents/'+agent_id+'/_execute'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, json=payload , headers=headers)\n",
    "\n",
    "if r.status_code == 200:\n",
    "    llm_agent_gen = json.loads(r.text)['inference_results'][0]['output'][0]['result'][0]\n",
    "    print(str(llm_agent_gen))\n",
    "\n",
    "\n",
    "#print status of the API call.\n",
    "print(f\"Status: {r.status_code}. Response:{r.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51a385",
   "metadata": {},
   "source": [
    "# Streamlit application deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa4951",
   "metadata": {},
   "source": [
    "## Write our code to app.py in the local file system\n",
    "\n",
    "Run the code block below to write its contents to app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fe9ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile ../../app.py\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "# import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "# from sagemaker import get_execution_role\n",
    "# import random \n",
    "# import string\n",
    "# import s3fs\n",
    "# from urllib.parse import urlparse\n",
    "# from IPython.display import display # , HTML\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests \n",
    "from PIL import Image\n",
    "import streamlit as st\n",
    "\n",
    "temp_dir = \"/home/ec2-user/SageMaker/advanced-rag-amazon-opensearch/retrieval-augment-generation/\"\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "st.title('Vector search with Amazon OpenSearch Service')\n",
    "\n",
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"multimodal-rag-opensearch\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "## Create a connection with OpenSearch domain.\n",
    "# Retrieving credentials from Secrets manager¶\n",
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['OpenSearchSecret'])['SecretString'])\n",
    "\n",
    "#credentials = boto3.Session().get_credentials()\n",
    "#auth = AWSV4SignerAuth(credentials, region)\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "ml_client = MLCommonClient(aos_client)\n",
    "#initializing some variables that we will use later.\n",
    "embedding_connector_id = \"\"\n",
    "embedding_model_id = \"\"\n",
    "\n",
    "# Create the OpenSearch Bedrock ML connector¶\n",
    "if not embedding_connector_id:\n",
    "    host = f'https://{aos_host}/'\n",
    "    service = 'es'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "\n",
    "    # Register repository\n",
    "    path = '_plugins/_ml/connectors/_create'\n",
    "    url = host + path\n",
    "\n",
    "    payload = {\n",
    "        \"name\": \"Amazon Bedrock Connector: embedding\",\n",
    "        \"description\": \"The connector to bedrock Titan multimodal embedding model\",\n",
    "        \"version\": 1,\n",
    "        \"protocol\": \"aws_sigv4\",\n",
    "        \"credential\": {\n",
    "          \"roleArn\": f\"arn:aws:iam::{account_id}:role/{bedrock_inf_iam_role}\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "        \"region\": region,\n",
    "        \"service_name\": \"bedrock\",\n",
    "        \"model\": \"amazon.titan-embed-image-v1\"\n",
    "       },\n",
    "       \"actions\": [\n",
    "        {\n",
    "          \"action_type\": \"predict\",\n",
    "          \"method\": \"POST\",\n",
    "          \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "          \"headers\": {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"x-amz-content-sha256\": \"required\"\n",
    "          },\n",
    "         \"request_body\": \"{ \\\"inputText\\\": \\\"${parameters.inputText}\\\" }\",\n",
    "         \"pre_process_function\": \"connector.pre_process.bedrock.embedding\",\n",
    "         \"post_process_function\": \"connector.post_process.bedrock.embedding\"}\n",
    "       ]\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        st.write(r.status_code)\n",
    "    embedding_connector_id = json.loads(r.text)[\"connector_id\"]\n",
    "else:\n",
    "    st.write(f\"Connector already exists - {embedding_connector_id}\")\n",
    "\n",
    "#tab1, right_column = st.columns(2)\n",
    "\n",
    "tab1, tab2 = st.tabs([\"Keyword Search\", \"Multimodal Search\"])\n",
    "\n",
    "with tab1:\n",
    "    st.header(\"Keyword Search\")\n",
    "    # st.image(\"https://static.streamlit.io/examples/cat.jpg\", width=200)\n",
    "with tab2:\n",
    "    st.header(\"Multimodal Search\")\n",
    "    # st.image(\"https://static.streamlit.io/examples/dog.jpg\", width=200)\n",
    "\n",
    "# Text search\n",
    "# tab1.header(\"Keyword Search\")\n",
    "query_1 = tab1.text_input(\"Keywords\",placeholder=\"Type your search here.\")\n",
    "url_1 = 'https://' + aos_host + \"/bedrock-multimodal-rag/_search\"\n",
    "keyword_payload_1 = {\"_source\": {\n",
    "        \"exclude\": [\n",
    "            \"vector_embedding\"\n",
    "        ]\n",
    "        },\n",
    "        \"query\": {    \"match\": {\n",
    "                        \"product_description\": {\n",
    "                            \"query\": query_1\n",
    "                        }\n",
    "                        }\n",
    "                    }\n",
    "        ,\"size\":5,\n",
    "  }\n",
    "\n",
    "r_1 = requests.get(url_1, auth=awsauth, json=keyword_payload_1, headers=headers)\n",
    "response_1 = json.loads(r_1.text)\n",
    "docs_1 = response_1['hits']['hits']\n",
    "\n",
    "for i,doc in enumerate(docs_1):\n",
    "    tab1.write(str(i+1)+ \". \"+doc[\"_source\"][\"product_description\"])\n",
    "    image = Image.open(temp_dir + doc[\"_source\"][\"image_url\"])\n",
    "    tab1.image(image)\n",
    "\n",
    "# Multimodal search with both image and text caption as inputs\n",
    "tab2.header(\"Multimodal search with both image and text caption as inputs\")\n",
    "embedding_model_id = tab2.text_input(\"Embedding model ID\", placeholder=\"embedding_model_id\", help=\"This is the embedding model you registered in the Jupyter notebook steps.\")\n",
    "query_2 = tab2.text_input(\"Keywords\", placeholder=\"Type your search text here.\")\n",
    "query_2_image = tab2.file_uploader(\"Image\", type=['png','jpeg','jpg'])\n",
    "\n",
    "if not (embedding_model_id and query_2):\n",
    "    st.stop()\n",
    "\n",
    "#Multimodal Search\n",
    "#Text as input\n",
    "url_2 = 'https://'+aos_host+\"/bedrock-multimodal-rag/_search\"\n",
    "text_embedding_payload_2 = {\"_source\": {\n",
    "        \"exclude\": [\n",
    "            \"vector_embedding\"\n",
    "        ]\n",
    "        },\n",
    "        \"query\": {    \n",
    "       \n",
    "        \"neural\": {\n",
    "            \"vector_embedding\": {\n",
    "                \n",
    "            #\"query_image\":query_image_binary,\n",
    "            \"query_text\":query_2,\n",
    "                \n",
    "            \"model_id\": embedding_model_id,\n",
    "            \"k\": 3\n",
    "            }\n",
    "            }\n",
    "            \n",
    "                    }\n",
    "        \n",
    "        ,\"size\":5,\n",
    "  }\n",
    "\n",
    "r_2 = requests.get(url_2, auth=awsauth, json=text_embedding_payload_2, headers=headers)\n",
    "response_2 = json.loads(r_2.text)\n",
    "docs_2 = response_2['hits']['hits']\n",
    "\n",
    "for i,doc in enumerate(docs_2):\n",
    "    tab2.write(str(i+1)+ \". \"+doc[\"_source\"][\"product_description\"])\n",
    "    image = Image.open(temp_dir + doc[\"_source\"][\"image_url\"])\n",
    "    tab2.image(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8954efe4",
   "metadata": {},
   "source": [
    "### Run the code blocks below to install dependencies and launch the Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit\n",
    "!pip install opensearch-py\n",
    "!pip install opensearch_py_ml\n",
    "!pip install deprecated\n",
    "!pip install requests_aws4auth\n",
    "!pip install requests\n",
    "!pip install PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c344cc",
   "metadata": {},
   "source": [
    "Once the pip installs complete, run the following cell to launch the Streamlit application server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c7597",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py --server.baseUrlPath=\"/proxy/absolute/8501\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c18f9",
   "metadata": {},
   "source": [
    "[Application link](https://semantic-search-nb-qnru.notebook.us-east-1.sagemaker.aws/proxy/absolute/8501/)\n",
    "https://semantic-search-nb-qnru.notebook.us-east-1.sagemaker.aws/proxy/absolute/8501/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
