{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082aa845",
   "metadata": {},
   "source": [
    "# Build Multimodal RAG with Amazon OpenSearch Service - to share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7b731",
   "metadata": {},
   "source": [
    "In this notebook, you will build and run multimodal search using a sample retail dataset. You will use multimodal generated embeddings for text and image and experiment by running text search only, image search only and both text and image search in OpenSearch Service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f69f6",
   "metadata": {},
   "source": [
    "You will be using a retail dataset that contains 2,465 retail product samples that belong to different categories such as accessories, home decor, apparel, housewares, books, and instruments. Each product contains metadata including the ID, current stock, name, category, style, description, price, image URL, and gender affinity of the product. You will be using only the product image and product description fields in the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ae065",
   "metadata": {},
   "source": [
    " \n",
    "Step 1: Create embeddings for text and images\n",
    "\n",
    "Step 2: Store the embeddings in OpenSearch Service index\n",
    "\n",
    "Step 3: Use LLM to generate text using the context from OpenSearch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Step 1: \n",
    "\n",
    "1. Build AI/connector between AOS and Embedding model - Titan Mulitmodal embeddings model\n",
    "2. Register/Deploy the Embedding model in AOS\n",
    "3. Create a KNN index in AOS\n",
    "4. Create an ingest pipeline to generate the embedding inside AOS\n",
    "\n",
    "Step 2:\n",
    "\n",
    "1. Index the data\n",
    "\n",
    "Step 3:\n",
    "\n",
    "1. Run multimodal neural search query in AOS \n",
    "2. Feed the LLM with the extract results from AOS - Claude Sonnet 3 \n",
    "2.1. Build AI/connector between AOS and LLM to generate the text\n",
    "2.2. Register/Deploy the LLM in AOS\n",
    "2.3. Run conversational search query in AOS (using the LLM model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be245f",
   "metadata": {},
   "source": [
    "## 1. Lab Pre-requisites\n",
    "\n",
    "\n",
    "For this notebook we require a few libraries. We'll use the Python clients for Amazon OpenSearch Service and Amazon Bedrock, and OpenSearch ML Client library for generating multimodal embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df92a75",
   "metadata": {},
   "source": [
    "#### 1.1. Import libraries & initialize resource information\n",
    "The line below will import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opensearch-py\n",
    "!pip install opensearch_py_ml\n",
    "!pip install deprecated\n",
    "!pip install requests_aws4auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d38375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import requests \n",
    "from requests_aws4auth import AWS4Auth\n",
    "from ruamel.yaml import YAML\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240112b",
   "metadata": {},
   "source": [
    "#### 1.2. Get CloudFormation stack output variables\n",
    "\n",
    "We have preconfigured a few resources by creating a CloudFormation stack in the account. Names and ARN of these resources will be used within this lab. We are going to load some of the information variables here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4f51e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"multimodal-rag-opensearch\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699df5a6",
   "metadata": {},
   "source": [
    "#### 1.3. Request Bedrock multimodal embedding model access in AWS Console\n",
    "\n",
    "Make sure you have access to \"Titan Multimodal Embeddings G1\" in Amazon Bedrock within the right AWS region\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b636855",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1eae6",
   "metadata": {},
   "source": [
    "### 2.1.Download the dataset (.gz) and extract the .gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp/images', exist_ok = True)\n",
    "metadata_file = urllib.request.urlretrieve('https://aws-blogs-artifacts-public.s3.amazonaws.com/BDB-3144/products-data.yml', 'tmp/images/products.yaml')\n",
    "img_filename,headers= urllib.request.urlretrieve('https://aws-blogs-artifacts-public.s3.amazonaws.com/BDB-3144/images.tar.gz', 'tmp/images/images.tar.gz')              \n",
    "print(img_filename)\n",
    "file = tarfile.open('tmp/images/images.tar.gz')\n",
    "file.extractall('tmp/images/')\n",
    "file.close()\n",
    "#remove images.tar.gz\n",
    "os.remove('tmp/images/images.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f307e7",
   "metadata": {},
   "source": [
    "## 3. Create a connection with OpenSearch domain.\n",
    "Next, we'll use Python API to set up connection with OpenSearch domain.\n",
    "\n",
    "#### Important pre-requisite\n",
    "You should have followed the steps in the Lab instruction section to map Sagemaker notebook role to OpenSearch `ml_full_access` role. If not, please visit the lab instructions and complete the **Setting up permission for Notebook IAM Role** section.\n",
    "\n",
    "#### Retrieving credentials from Secrets manager\n",
    "We are going to use Amazon Sagemaker Notebook IAM role to configure the workflows in OpenSearch. This IAM Role has permission to pass BedrockInference IAM role to OpenSearch. OpenSearch will then be able to use BedrockInference IAM role to make calls to Bedrock models.\n",
    "\n",
    "##### NOTE: \n",
    "_At any point in this lab, if you get a failure message - **The security token included in the request is expired.**_ You can resolve it by running this cell again. The cell refreshes the security credentials that is required for the rest of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21309d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['OpenSearchSecret'])['SecretString'])\n",
    "\n",
    "#credentials = boto3.Session().get_credentials()\n",
    "#auth = AWSV4SignerAuth(credentials, region)\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "ml_client = MLCommonClient(aos_client)\n",
    "\n",
    "#initializing some variables that we will use later.\n",
    "\n",
    "embedding_connector_id = \"\"\n",
    "embedding_model_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f468131",
   "metadata": {},
   "source": [
    "## 4. Create the OpenSearch Bedrock ML connector to Amazon Bedrock Titan Multimodal Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a113cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an OpenSearch remote model connector with Amazon Bedrock Titan MM Embedding model.\n",
    "\n",
    "if not embedding_connector_id:\n",
    "    host = f'https://{aos_host}/'\n",
    "    service = 'es'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "\n",
    "    # Register repository\n",
    "    path = '_plugins/_ml/connectors/_create'\n",
    "    url = host + path\n",
    "\n",
    "    payload = {\n",
    "        \"name\": \"Amazon Bedrock Connector: embedding\",\n",
    "        \"description\": \"The connector to bedrock Titan multimodal embedding model\",\n",
    "        \"version\": 1,\n",
    "        \"protocol\": \"aws_sigv4\",\n",
    "        \"credential\": {\n",
    "          \"roleArn\": f\"arn:aws:iam::{account_id}:role/{bedrock_inf_iam_role}\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "        \"region\": region,\n",
    "        \"service_name\": \"bedrock\",\n",
    "        \"model\": \"amazon.titan-embed-image-v1\"\n",
    "       },\n",
    "       \"actions\": [\n",
    "        {\n",
    "          \"action_type\": \"predict\",\n",
    "          \"method\": \"POST\",\n",
    "          \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "          \"headers\": {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"x-amz-content-sha256\": \"required\"\n",
    "          },\n",
    "         \"request_body\": \"{ \\\"inputText\\\": \\\"${parameters.inputText:-null}\\\", \\\"inputImage\\\": \\\"${parameters.inputImage:-null}\\\" }\",\n",
    "      \"pre_process_function\": \"\\n    StringBuilder parametersBuilder = new StringBuilder(\\\"{\\\");\\n    if (params.text_docs.length > 0 && params.text_docs[0] != null) {\\n      parametersBuilder.append(\\\"\\\\\\\"inputText\\\\\\\":\\\");\\n      parametersBuilder.append(\\\"\\\\\\\"\\\");\\n      parametersBuilder.append(params.text_docs[0]);\\n      parametersBuilder.append(\\\"\\\\\\\"\\\");\\n      \\n      if (params.text_docs.length > 1 && params.text_docs[1] != null) {\\n        parametersBuilder.append(\\\",\\\");\\n      }\\n    }\\n    \\n    \\n    if (params.text_docs.length > 1 && params.text_docs[1] != null) {\\n      parametersBuilder.append(\\\"\\\\\\\"inputImage\\\\\\\":\\\");\\n      parametersBuilder.append(\\\"\\\\\\\"\\\");\\n      parametersBuilder.append(params.text_docs[1]);\\n      parametersBuilder.append(\\\"\\\\\\\"\\\");\\n    }\\n    parametersBuilder.append(\\\"}\\\");\\n    \\n    return  \\\"{\\\" +\\\"\\\\\\\"parameters\\\\\\\":\\\" + parametersBuilder + \\\"}\\\";\",\n",
    "      \"post_process_function\": \"\\n      def name = \\\"sentence_embedding\\\";\\n      def dataType = \\\"FLOAT32\\\";\\n      if (params.embedding == null || params.embedding.length == 0) {\\n          return null;\\n      }\\n      def shape = [params.embedding.length];\\n      def json = \\\"{\\\" +\\n                 \\\"\\\\\\\"name\\\\\\\":\\\\\\\"\\\" + name + \\\"\\\\\\\",\\\" +\\n                 \\\"\\\\\\\"data_type\\\\\\\":\\\\\\\"\\\" + dataType + \\\"\\\\\\\",\\\" +\\n                 \\\"\\\\\\\"shape\\\\\\\":\\\" + shape + \\\",\\\" +\\n                 \\\"\\\\\\\"data\\\\\\\":\\\" + params.embedding +\\n                 \\\"}\\\";\\n      return json;\\n    \"\n",
    "    }\n",
    "  ] \n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    print(r.status_code)\n",
    "    print(r.text)\n",
    "    embedding_connector_id = json.loads(r.text)[\"connector_id\"]\n",
    "else:\n",
    "    print(f\"Connector already exists - {embedding_connector_id}\")\n",
    "    \n",
    "embedding_connector_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eed3ef",
   "metadata": {},
   "source": [
    "Once the model connector is defined. We need to register the model and deploy. Following two cells will register and then deploy the model connection respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the multimodal embedding model\n",
    "if not embedding_model_id:\n",
    "    path = '_plugins/_ml/models/_register'\n",
    "    url = 'https://'+aos_host + '/' + path\n",
    "    payload = { \"name\": \"Bedrock Titan mm embeddings model\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"Bedrock Titan mm embeddings model\",\n",
    "    \"connector_id\": embedding_connector_id}\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    embedding_model_id = json.loads(r.text)[\"model_id\"]\n",
    "else:\n",
    "    print(\"skipping model registration - model already exists\")\n",
    "print(\"Model registered under model_id: \"+embedding_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387db013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the embedding model\n",
    "path = '_plugins/_ml/models/'+embedding_model_id+'/_deploy'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, headers=headers)\n",
    "deploy_status = json.loads(r.text)[\"status\"]\n",
    "print(\"Deployment status of the model, \"+embedding_model_id+\" : \"+deploy_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852500c8",
   "metadata": {},
   "source": [
    "## 4. Test the OpenSearch - Bedrock integration with a test input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6153286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "\n",
    "\n",
    "path = '_plugins/_ml/models/'+embedding_model_id+'/_predict'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "img = \"tmp/images/footwear/2d2d8ec8-4806-42a7-b8ba-ceb15c1c7e84.jpg\"\n",
    "with open(img, \"rb\") as image_file:\n",
    "    input_image_binary = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "    \n",
    "payload = {\n",
    "\"parameters\": {\n",
    "\"inputText\": \"Sleek, stylish black sneakers made for urban exploration. With fashionable looks and comfortable design, these sneakers keep your feet looking great while you walk the city streets in style\",\n",
    "\"inputImage\":input_image_binary\n",
    "}\n",
    "}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "\n",
    "try:\n",
    "    embed = json.loads(r.text)['inference_results'][0]['output'][0]['data'][0:10]\n",
    "    shape = json.loads(r.text)['inference_results'][0]['output'][0]['shape'][0]\n",
    "    print(\"First 10 dimensions:\")\n",
    "    print(str(embed))\n",
    "    print(\"\\n\")\n",
    "    print(\"Total: \" + str(shape) + \" dimensions\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")\n",
    "    print(\"The response does not contain the expected data structure.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"An unexpected error occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd7e48",
   "metadata": {},
   "source": [
    "## 5. Create the OpenSearch ingest pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d48551",
   "metadata": {},
   "source": [
    "\n",
    "Let's create an ingestion pipeline that will call Amazon Bedrock Titan Multimodal embedding model and convert the text and image into multimodal vector embedding. Ingest pipeline is a feature in OpenSearch that allows you to define certain actions to be performed at the time of data ingestion. You could do simple processing such as adding a static field, modify an existing field, or call a remote model to get inference and store inference output together with the indexed record/document. In our case inference output is vector embedding.\n",
    "\n",
    "Following ingestion pipeline is going to call our remote model and convert product image `product_description` field and the `image_binary` to vector and store it in the field called `vector_embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"_ingest/pipeline/bedrock-multimodal-ingest-pipeline\"\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "payload = {\n",
    "\"description\": \"A text/image embedding pipeline\",\n",
    "\"processors\": [\n",
    "{\n",
    "\"text_image_embedding\": {\n",
    "\"model_id\":embedding_model_id,\n",
    "\"embedding\": \"vector_embedding\",\n",
    "\"field_map\": {\n",
    "\"text\": \"product_description\",\n",
    "\"image\": \"image_binary\"\n",
    "}}}]}\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f358ebf",
   "metadata": {},
   "source": [
    "## 6. Create the k-NN index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"bedrock-multimodal-rag\"\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "\n",
    "#this will delete the index if already exists\n",
    "requests.delete(url, auth=awsauth, json=payload, headers=headers)\n",
    "\n",
    "payload = {\n",
    "  \"settings\": {\n",
    "    \"index.knn\": True,\n",
    "    \"default_pipeline\": \"bedrock-multimodal-ingest-pipeline\"\n",
    "  },\n",
    "  \"mappings\": {\n",
    "      \n",
    "    \"_source\": {\n",
    "     \n",
    "    },\n",
    "    \"properties\": {\n",
    "      \"vector_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": shape,\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"faiss\",\n",
    "          \"parameters\": {}\n",
    "        }\n",
    "      },\n",
    "      \"product_description\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "        \"image_url\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"image_binary\": {\n",
    "        \"type\": \"binary\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42df6cf",
   "metadata": {},
   "source": [
    "## 7. Ingest the dataset into k-NN index usig Bulk request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea00f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resize_image(photo, width, height):\n",
    "    Image.MAX_IMAGE_PIXELS = 100000000\n",
    "    \n",
    "    with Image.open(photo) as image:\n",
    "        image.verify()\n",
    "    with Image.open(photo) as image:    \n",
    "        \n",
    "        if image.format in [\"JPEG\", \"PNG\"]:\n",
    "            file_type = image.format.lower()\n",
    "            path = image.filename.rsplit(\".\", 1)[0]\n",
    "\n",
    "            image.thumbnail((width, height))\n",
    "            image.save(f\"{path}-resized.{file_type}\")\n",
    "    return file_type, path\n",
    "\n",
    "# Load the products from the dataset\n",
    "yaml = YAML()\n",
    "items_ = yaml.load(open('tmp/images/products.yaml'))\n",
    "\n",
    "batch = 0\n",
    "count = 0\n",
    "body_ = ''\n",
    "batch_size = 100\n",
    "last_batch = int(len(items_)/batch_size)\n",
    "action = json.dumps({ 'index': { '_index': 'bedrock-multimodal-rag' } })\n",
    "\n",
    "for item in items_:\n",
    "    count+=1\n",
    "    fileshort = \"tmp/images/\"+item[\"category\"]+\"/\"+item[\"image\"]\n",
    "    payload = {}\n",
    "    payload['image_url'] = fileshort\n",
    "    payload['product_description'] = item['description']\n",
    "    \n",
    "    #resize the image and generate image binary\n",
    "    file_type, path = resize_image(fileshort, 2048, 2048)\n",
    "\n",
    "    with open(fileshort.split(\".\")[0]+\"-resized.\"+file_type, \"rb\") as image_file:\n",
    "        input_image = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "    \n",
    "    os.remove(fileshort.split(\".\")[0]+\"-resized.\"+file_type)\n",
    "    payload['image_binary'] = input_image\n",
    "    \n",
    "    body_ = body_ + action + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "    \n",
    "    if(count == batch_size):\n",
    "        response = aos_client.bulk(\n",
    "        index = \"bedrock-multimodal-rag\",\n",
    "        body = body_\n",
    "        )\n",
    "        batch += 1\n",
    "        count = 0\n",
    "        print(\"batch \"+str(batch) + \" ingestion done!\")\n",
    "        if(batch != last_batch):\n",
    "            body_ = \"\"\n",
    "        \n",
    "            \n",
    "#ingest the remaining rows\n",
    "response = aos_client.bulk(\n",
    "        index = \"bedrock-multimodal-rag\",\n",
    "        body = body_\n",
    "        )\n",
    "        \n",
    "print(\"All \"+str(last_batch)+\" batches ingested into index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = aos_client.search(index=\"bedrock-multimodal-rag\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b08140",
   "metadata": {},
   "source": [
    "## 8. Lexical search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbb176",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Keyword Search\n",
    "query = \"trendy footwear for women\"\n",
    "url = 'https://' + aos_host + \"/bedrock-multimodal-rag/_search\"\n",
    "keyword_payload = {\"_source\": {\n",
    "        \"exclude\": [\n",
    "            \"vector_embedding\"\n",
    "        ]\n",
    "        },\n",
    "        \"query\": {    \"match\": {\n",
    "                        \"product_description\": {\n",
    "                            \"query\": query\n",
    "                        }\n",
    "                        }\n",
    "                    }\n",
    "        \n",
    "        ,\"size\":5,\n",
    "  }\n",
    "\n",
    "r = requests.get(url, auth=awsauth, json=keyword_payload, headers=headers)\n",
    "response_ = json.loads(r.text)\n",
    "docs = response_['hits']['hits']\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(str(i+1)+ \". \"+doc[\"_source\"][\"product_description\"])\n",
    "    image = Image.open(doc[\"_source\"][\"image_url\"])\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1cb55",
   "metadata": {},
   "source": [
    "## 9. Multimodal search with both image and text caption as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce08e5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Multimodal Search\n",
    "#Text and image as inputs\n",
    "s3 = boto3.client('s3')\n",
    "url = 'https://'+aos_host + \"/bedrock-multimodal-rag/_search\"\n",
    "query = \"trendy footwear for women\"\n",
    "print(\"Input text query: \"+query)\n",
    "# urllib.request.urlretrieve( \n",
    "#   'https://cdn.pixabay.com/photo/2014/09/03/20/15/shoes-434918_1280.jpg',\"tmp/women-footwear.jpg\") \n",
    "img = Image.open(\"tmp/women-footwear-1.jpg\") \n",
    "print(\"Input query Image:\")\n",
    "img.show()\n",
    "with open(\"tmp/women-footwear-1.jpg\", \"rb\") as image_file:\n",
    "    query_image_binary = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "keyword_payload = {\"_source\": {\n",
    "        \"exclude\": [\n",
    "            \"vector_embedding\"\n",
    "        ]\n",
    "        },\n",
    "        \"query\": {    \n",
    "       \n",
    "        \"neural\": {\n",
    "            \"vector_embedding\": {\n",
    "                \n",
    "            \"query_image\":query_image_binary,\n",
    "            \"query_text\":query,\n",
    "                \n",
    "            \"model_id\": embedding_model_id,\n",
    "            \"k\": 5\n",
    "            }\n",
    "            \n",
    "            }\n",
    "                    }\n",
    "        \n",
    "        ,\"size\":5,\n",
    "  }\n",
    "\n",
    "r = requests.get(url, auth=awsauth, json=keyword_payload, headers=headers)\n",
    "response_ = json.loads(r.text)\n",
    "docs = response_['hits']['hits']\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(doc[\"_source\"][\"product_description\"])\n",
    "    image = Image.open(doc[\"_source\"][\"image_url\"])\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77feb61e",
   "metadata": {},
   "source": [
    "# Multimodal Conversational search\n",
    "\n",
    "In this section, you will be using OpenSearch Service as the knowledge database to run multimodal retrieval and augment the LLM prompt with the relevant context. You will be using Claude v3 Sonnet as the foundational model to generate response and provide fashion advice to end users based on the available retail items stored in OpenSearch Service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1a87f",
   "metadata": {},
   "source": [
    "## 10. Create the OpenSearch Bedrock Claude LLM connector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ed154",
   "metadata": {},
   "source": [
    "## Create an OpenSearch AI connector for Claude V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6300507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing variables that we will use later.\n",
    "\n",
    "llm_connector_id = \"\"\n",
    "llm_model_id = \"\"\n",
    "\n",
    "if not llm_connector_id:\n",
    "    host = f'https://{aos_host}/'\n",
    "    service = 'es'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "\n",
    "    # Register repository\n",
    "    path = '_plugins/_ml/connectors/_create'\n",
    "    url = host + path\n",
    "\n",
    "    payload = {\n",
    "        \"name\": \"Amazon Bedrock Connector: Claude 2\",\n",
    "        \"description\": \"The connector to bedrock Claude V2\",\n",
    "        \"version\": 1,\n",
    "        \"protocol\": \"aws_sigv4\",\n",
    "        \"credential\": {\n",
    "          \"roleArn\": f\"arn:aws:iam::{account_id}:role/{bedrock_inf_iam_role}\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "           \"region\": region,\n",
    "        \"service_name\": \"bedrock\",\n",
    "        \"auth\": \"Sig_V4\",\n",
    "        \"model\": \"anthropic.claude-v2\"\n",
    "       },\n",
    "       \"actions\": [\n",
    "        {\n",
    "          \"action_type\": \"predict\",\n",
    "          \"method\": \"POST\",\n",
    "          \"headers\": {\n",
    "            \"content-type\": \"application/json\"\n",
    "          },\n",
    "            \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "            \"request_body\": \"{\\\"prompt\\\":\\\"\\\\n\\\\nHuman: ${parameters.inputs}\\\\n\\\\nAssistant:\\\",\\\"max_tokens_to_sample\\\":300,\\\"temperature\\\":0.5,\\\"top_k\\\":250,\\\"top_p\\\":1,\\\"stop_sequences\\\":[\\\"\\\\\\\\n\\\\\\\\nHuman:\\\"]}\"\n",
    "        }\n",
    "            ]\n",
    "    }\n",
    "\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    print(r.status_code)\n",
    "    print(r.text)\n",
    "    llm_connector_id = json.loads(r.text)[\"connector_id\"]\n",
    "else:\n",
    "    print(f\"Connector already exists - {llm_connector_id}\")\n",
    "    \n",
    "llm_connector_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6123b4",
   "metadata": {},
   "source": [
    "## 11. Register and deploy the OpenSearch Bedrock Claude LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register and deploy the llm model\n",
    "if not llm_model_id:\n",
    "    path = '_plugins/_ml/models/_register?deploy=true'\n",
    "    url = 'https://'+aos_host + '/' + path\n",
    "    payload = { \n",
    "        \"name\": \"Amazon Bedrock Connector: Claude v2\",\n",
    "        \"function_name\": \"remote\",\n",
    "        \"description\": \"The connector to bedrock Claude v2\",\n",
    "        \"connector_id\": llm_connector_id\n",
    "    }\n",
    "    \n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    llm_model_id = json.loads(r.text)[\"model_id\"]\n",
    "    \n",
    "else:\n",
    "    print(\"skipping model registration - llm model already exists\")\n",
    "print(\"Model registered and deployed under llm_model_id: \"+llm_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb824b38",
   "metadata": {},
   "source": [
    "## 12. Test LLM model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb698c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '_plugins/_ml/models/'+llm_model_id+'/_predict'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "\n",
    "payload = {\n",
    "    \n",
    "    \"parameters\": {\n",
    "    \"inputs\": \"what do you recommend as outdoors footwear for long walk?\"\n",
    "  }\n",
    "}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    \n",
    "if r.status_code == 200:\n",
    "    llm_gen = json.loads(r.text)['inference_results'][0]['output'][0]['dataAsMap']['completion']\n",
    "    print(str(\"Claude generated response without context: \\n\\n\"+llm_gen))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e31ef",
   "metadata": {},
   "source": [
    "## 13. Create conversational search using neural search in OpenSearch Service\n",
    "\n",
    "Conversational search lets you ask questions in natural language, receive a text response based on a provided context and ask additional clarifying questions. \n",
    "\n",
    "In order to have a conversational search, the LLM needs to remember the context of the entire conversation to have follow-up questions. This is composed of 2 components:\n",
    "\n",
    "Part 1: Define a conversation history comprising of all messages (The human-input question and the LLM answer) being stored within the same conversation memory.\n",
    "\n",
    "Part 2: Define a Retrieval-Augmented Generation (RAG) pipeline to augment the LLM prompt with the relevant context from OpenSearch Service vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723877d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluster prerequisites\n",
    "## Enable conversation memory and RAG pipeline features in your cluster\n",
    "\n",
    "\n",
    "path = '/_cluster/settings'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "\n",
    "payload = {\n",
    "    \n",
    "    \"persistent\": {\n",
    "    \"plugins.ml_commons.memory_feature_enabled\": \"true\",\n",
    "    \"plugins.ml_commons.rag_pipeline_feature_enabled\": \"true\"\n",
    "  }\n",
    "}\n",
    "\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6075c",
   "metadata": {},
   "source": [
    "## 14. Create the OpenSearch search RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b377ecc",
   "metadata": {},
   "source": [
    "The RAG search pipeline uses the retrieval_augmented_generation processor used in conversational search to enables the integration of information retrieval and language generation for conversational search applications. \n",
    "\n",
    "The retrieval_augmented_generation processor is a search results processor that intercepts query results, retrieves previous messages from the conversation, and send a prompt to a large language model (LLM), saving the response in conversational memory and returning both the original OpenSearch query results and the LLM response.\n",
    "\n",
    "Let's create the search pipeline with the retrieval_augmented_generation processor to use at search time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2652814",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"_search/pipeline/multimodal_rag_pipeline\"\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "payload = {\n",
    "    \"response_processors\": [\n",
    "    {\n",
    "      \"retrieval_augmented_generation\": {\n",
    "        \"tag\": \"bedrock_rag-pipeline_demo\",\n",
    "        \"description\": \"Search pipeline using Bedrock Claude v2 Connector for RAG\",\n",
    "        \"model_id\": llm_model_id,\n",
    "        \"context_field_list\": [\"product_description\"],\n",
    "        \"system_prompt\": \"You are a helpful shopping advisor that uses their vast knowledge of fashion tips to make great recommendations people will enjoy.\",\n",
    "        \"user_instructions\": \"As a shopping advisor, be friendly and approachable. Greet the customer warmly. Evaluate each item provided in the context and provide a concise recommendation about each item to matches best the customer question using the order and number of search result related to each item. If there are items in the provided context that do not match the user question, explain that this may be due to insufficient items in the inventory. Finally, thank the client and let them know you're available if they have any other questions.\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "\n",
    "\n",
    "}\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7fa3e",
   "metadata": {},
   "source": [
    "## 15. RAG using multimodal search to provide prompt context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5debf27c",
   "metadata": {},
   "source": [
    "### First start by creating a conversational memory to store the 5 most recent messages to augment the LLM prompt with the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b041545",
   "metadata": {},
   "source": [
    "Before starting a converstation, run the cell below to create the conversational memory. Do not run the cell below if you would like to run follow up questions whithin the same conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f575daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a conversation memory\n",
    "\n",
    "## Initialize memory id variable to use in the next conversational search\n",
    "\n",
    "memory_id= ''\n",
    "\n",
    "path = '/_plugins/_ml/memory/'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "\n",
    "payload = {\n",
    "    \n",
    "    \"name\": \"Conversation about bags\"\n",
    "}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "\n",
    "memory_id = json.loads(r.text)[\"memory_id\"]\n",
    "\n",
    "print(\"The new created memory id: \" +memory_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cd07a",
   "metadata": {},
   "source": [
    "### Run a conversational search using the memory id created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"is this suitable for long walk?\"\n",
    "url = 'https://' + aos_host + \"/bedrock-multimodal-rag/_search?search_pipeline=multimodal_rag_pipeline\"\n",
    "\n",
    "\n",
    "print(\"Input text query: \"+query)\n",
    "# urllib.request.urlretrieve( \n",
    "#   'https://cdn.pixabay.com/photo/2014/09/03/20/15/shoes-434918_1280.jpg',\"tmp/women-footwear.jpg\") \n",
    "img = Image.open(\"tmp/images/accessories/1.jpg\") \n",
    "print(\"Input query Image:\")\n",
    "img.show()\n",
    "with open(\"tmp/images/accessories/1.jpg\", \"rb\") as image_file:\n",
    "    query_image_binary = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "\n",
    "\n",
    "multimodal_payload = {\n",
    "    \"_source\": {\n",
    "        \"exclude\": [\n",
    "            \"vector_embedding\", \"image_binary\"\n",
    "        ]},\n",
    "    \"query\": {\n",
    "        \"neural\": {\n",
    "      \"vector_embedding\": {\n",
    "        \"query_image\":query_image_binary,\n",
    "        \"query_text\": query,\n",
    "        \"model_id\": embedding_model_id,\n",
    "        \"k\": 5\n",
    "      }\n",
    "    }\n",
    "          },\n",
    "    \"size\":5,\n",
    "    \"ext\": {\n",
    "    \"generative_qa_parameters\": {\n",
    "      \"llm_model\": \"bedrock/claude\",\n",
    "      \"llm_question\": query,\n",
    "     \"memory_id\": memory_id ,\n",
    "      \"context_size\": 5,\n",
    "      \"message_size\": 5,\n",
    "      \"timeout\": 60\n",
    "    }\n",
    "  }\n",
    "  }\n",
    "\n",
    "r = requests.get(url, auth=awsauth, json=multimodal_payload, headers=headers)\n",
    "response_ = json.loads(r.text)\n",
    "docs = response_['hits']['hits']\n",
    "rag = response_['ext']['retrieval_augmented_generation']\n",
    "\n",
    "print(rag['answer']+\"\\n\\n\")\n",
    "print(\"Context: \\n\\n\")\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(str(i+1)+ \". \"+doc[\"_source\"][\"product_description\"])\n",
    "    image = Image.open(doc[\"_source\"][\"image_url\"])\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c975b9",
   "metadata": {},
   "source": [
    "### Ask a follow up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ade453",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"thanks. How about similar bag for kids?\"\n",
    "url = 'https://' + aos_host + \"/bedrock-multimodal-rag/_search?search_pipeline=multimodal_rag_pipeline\"\n",
    "\n",
    "\n",
    "print(\"Input text query: \"+query)\n",
    "# urllib.request.urlretrieve( \n",
    "#   'https://cdn.pixabay.com/photo/2014/09/03/20/15/shoes-434918_1280.jpg',\"tmp/women-footwear.jpg\") \n",
    "img = Image.open(\"tmp/images/accessories/1.jpg\") \n",
    "print(\"Input query Image:\")\n",
    "img.show()\n",
    "with open(\"tmp/images/accessories/1.jpg\", \"rb\") as image_file:\n",
    "    query_image_binary = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "\n",
    "\n",
    "multimodal_payload = {\n",
    "    \"_source\": {\n",
    "        \"exclude\": [\n",
    "            \"vector_embedding\", \"image_binary\"\n",
    "        ]},\n",
    "    \"query\": {\n",
    "        \"neural\": {\n",
    "      \"vector_embedding\": {\n",
    "        \"query_image\":query_image_binary,\n",
    "        \"query_text\": query,\n",
    "        \"model_id\": embedding_model_id,\n",
    "        \"k\": 5\n",
    "      }\n",
    "    }\n",
    "          },\n",
    "    \"size\":5,\n",
    "    \"ext\": {\n",
    "    \"generative_qa_parameters\": {\n",
    "      \"llm_model\": \"bedrock/claude\",\n",
    "      \"llm_question\": query,\n",
    "     \"memory_id\": memory_id ,\n",
    "      \"context_size\": 5,\n",
    "      \"message_size\": 5,\n",
    "      \"timeout\": 60\n",
    "    }\n",
    "  }\n",
    "  }\n",
    "\n",
    "r = requests.get(url, auth=awsauth, json=multimodal_payload, headers=headers)\n",
    "response_ = json.loads(r.text)\n",
    "docs = response_['hits']['hits']\n",
    "rag = response_['ext']['retrieval_augmented_generation']\n",
    "\n",
    "print(rag['answer']+\"\\n\\n\")\n",
    "print(\"Context: \\n\\n\")\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    print(str(i+1)+ \". \"+doc[\"_source\"][\"product_description\"])\n",
    "    image = Image.open(doc[\"_source\"][\"image_url\"])\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914facab",
   "metadata": {},
   "source": [
    "## Check the conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679da5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To verify that the messages were added to the memory, provide the memory_ID to the Get Messages API:\n",
    "\n",
    "\n",
    "path = '/_plugins/_ml/memory/'+memory_id +'/messages'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "\n",
    "r = requests.get(url, auth=awsauth, json=payload, headers=headers)\n",
    "\n",
    "response = json.loads(r.text)\n",
    "\n",
    "#print(\"The response contains the following messages\" + r.text)\n",
    "\n",
    "print(json.dumps(response, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173ed59",
   "metadata": {},
   "source": [
    "# Optional - Agent RAG flow with OpenSearch Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991cb05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluster prerequisites\n",
    "## Enable agent framework in your cluster and disable triggering the native memory circuit breaker\n",
    "\n",
    "\n",
    "path = '/_cluster/settings'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "\n",
    "payload = {\n",
    "    \n",
    "    \"persistent\": {\n",
    "    \"plugins.ml_commons.native_memory_threshold\": 100,\n",
    "    \"plugins.ml_commons.agent_framework_enabled\": \"true\"\n",
    "  }\n",
    "}\n",
    "\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "\n",
    "\n",
    "if (r.status_code==200):\n",
    "    print(\"Agent framework is enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58de963",
   "metadata": {},
   "source": [
    "# Create connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing variables that we will use later.\n",
    "\n",
    "llm_agent_connector_id = \"\"\n",
    "llm_agent_model_id = \"\"\n",
    "\n",
    "if not llm_agent_connector_id:\n",
    "    host = f'https://{aos_host}/'\n",
    "    service = 'es'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "\n",
    "    # Register repository\n",
    "    path = '_plugins/_ml/connectors/_create'\n",
    "    url = host + path\n",
    "\n",
    "    payload = {\n",
    "        \"name\": \"Amazon Bedrock Connector: Claude v1\",\n",
    "        \"description\": \"The connector to bedrock Claude V1\",\n",
    "        \"version\": 1,\n",
    "        \"protocol\": \"aws_sigv4\",\n",
    "        \"credential\": {\n",
    "          \"roleArn\": f\"arn:aws:iam::{account_id}:role/{bedrock_inf_iam_role}\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "           \"region\": region,\n",
    "        \"service_name\": \"bedrock\",\n",
    "        \"auth\": \"Sig_V4\",\n",
    "        \"model\": \"anthropic.claude-instant-v1\",\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens_to_sample\": 8000,\n",
    "    \"temperature\": 0.0001,\n",
    "    \"response_filter\": \"$.completion\"\n",
    "       },\n",
    "       \"actions\": [\n",
    "        {\n",
    "          \"action_type\": \"predict\",\n",
    "          \"method\": \"POST\",\n",
    "          \"headers\": {\n",
    "            \"content-type\": \"application/json\",\n",
    "        \"x-amz-content-sha256\": \"required\"\n",
    "          },\n",
    "            \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "            \"request_body\": \"{\\\"prompt\\\":\\\"${parameters.prompt}\\\", \\\"max_tokens_to_sample\\\":${parameters.max_tokens_to_sample}, \\\"temperature\\\":${parameters.temperature},  \\\"anthropic_version\\\":\\\"${parameters.anthropic_version}\\\" }\"\n",
    "        }\n",
    "            ]\n",
    "    }\n",
    "\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    print(r.status_code)\n",
    "    llm_agent_connector_id = json.loads(r.text)[\"connector_id\"]\n",
    "else:\n",
    "    print(f\"Connector already exists - {llm_agent_connector_id}\")\n",
    "print(f\"llm_agent_connector_id : {llm_agent_connector_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register and deploy the llm model\n",
    "if not llm_agent_model_id:\n",
    "    path = '_plugins/_ml/models/_register?deploy=true'\n",
    "    url = 'https://'+aos_host + '/' + path\n",
    "    payload = { \n",
    "        \"name\": \"Amazon Bedrock Connector: Claude v1\",\n",
    "        \"function_name\": \"remote\",\n",
    "        \"description\": \"The connector to bedrock Claude v1\",\n",
    "        \"connector_id\": \"lBKGTZEBdCm6hxASedK7\"\n",
    "    }\n",
    "    \n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    llm_agent_model_id = json.loads(r.text)[\"model_id\"]\n",
    "    \n",
    "else:\n",
    "    print(\"skipping model registration - llm model already exists\")\n",
    "print(\"llm_agent_model_id: \"+llm_agent_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b8e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '_plugins/_ml/models/'+llm_agent_model_id+'/_predict'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "\n",
    "payload = {\n",
    "    \n",
    "    \"parameters\": {\n",
    "    \"prompt\": \"\\n\\nHuman: how are you? \\n\\nAssistant:\"\n",
    "  }\n",
    "}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "\n",
    "\n",
    "if r.status_code == 200:\n",
    "    llm_gen = json.loads(r.text)['inference_results'][0]['output'][0]['dataAsMap']['response']\n",
    "    print(llm_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb039c8",
   "metadata": {},
   "source": [
    "## 15. Create, register and execute an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing variable to store the agend id\n",
    "\n",
    "agent_id = \"\"\n",
    "\n",
    "\n",
    "# you’ll use the embedding model and the Claude model to create a flow agent\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"Fashion stylist agent\",\n",
    "  \"type\": \"conversational_flow\",\n",
    "  \"description\": \"this is a demo agent for fashion stylist\",\n",
    "  \"app_type\": \"rag\",\n",
    "     \"memory\": {\n",
    "        \"type\": \"conversation_index\"\n",
    "    },\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"type\": \"VectorDBTool\",\n",
    "        \"name\": \"shopping_knowledge_base\",\n",
    "      \"parameters\": {\n",
    "        \"model_id\": embedding_model_id,\n",
    "        \"index\": \"bedrock-multimodal-rag\",\n",
    "        \"embedding_field\": \"vector_embedding\",\n",
    "        \"source_field\": [\n",
    "            \"product_description\"\n",
    "        ],\n",
    "          \n",
    "        \"input\": \"${parameters.question}\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"MLModelTool\",\n",
    "        \"name\": \"bedrock_claude_model_v1\",\n",
    "      \"description\": \"A tool to recommend fashion styles\",\n",
    "      \"parameters\": {\n",
    "        \"model_id\": llm_agent_model_id,\n",
    "        \"prompt\": \"\\n\\nHuman:You are a professional fashion stylist. You will always recommend product based on the given context. If you don't have enough context, you will ask Human to provide more information. If you don't see any related product to recommend, just say we don't have such product. If you don't know the answer, just say you don't know. \\n\\nContext:\\n${parameters.shopping_knowledge_base.output:-}\\n\\n${parameters.chat_history:-}\\n\\nHuman:${parameters.question}\\n\\nAssistant:\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "path = '_plugins/_ml/agents/_register'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, json=payload , headers=headers )\n",
    "\n",
    "agent_id = json.loads(r.text)[\"agent_id\"]\n",
    "\n",
    "#print status of the API call.\n",
    "print(f\"Status: {r.status_code}. Response:{r.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c095ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can inspect the agent by sending a request to the agents endpoint and providing the agent ID:\n",
    "\n",
    "\n",
    "# You can test the llm text generation here.\n",
    "agent_payload = {\n",
    "    \"parameters\":\n",
    "    {\n",
    "        \"question\": \"What are the most comfortable shoes\",\n",
    "    \"verbose\": \"true\"\n",
    "    } \n",
    "}\n",
    "\n",
    "print((r.text))\n",
    "\n",
    "path = '_plugins/_ml/agents/'+agent_id+'/_execute'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, json=agent_payload , headers=headers)\n",
    "\n",
    "if r.status_code == 200:\n",
    "    llm_agent_gen = json.loads(r.text)['inference_results'][0]['output'][2]['result']\n",
    "    print(str(llm_agent_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08dd75",
   "metadata": {},
   "source": [
    "# Deploy your application\n",
    "\n",
    "To deploy this code as an application we will use [Streamlit](https://streamlit.io/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669597e1",
   "metadata": {},
   "source": [
    "**Step 1:** Export the connector and model IDs you created earlier in this notebook. We will store them to a file so that they can be referenced outside of the notebook and persisted between kernel restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "connector_ids = {}\n",
    "connector_ids['llm_connector_id'] = llm_connector_id\n",
    "connector_ids['llm_model_id'] = llm_model_id\n",
    "connector_ids['embedding_connector_id'] = embedding_connector_id\n",
    "connector_ids['embedding_model_id'] = embedding_model_id\n",
    "connector_ids['agent_id'] = agent_id\n",
    "\n",
    "with open('connector_ids.json', 'w') as file:\n",
    "    json.dump(connector_ids, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc06a1e",
   "metadata": {},
   "source": [
    "**Step 2:** Since you will launch the app.py script using the Notebook instance's shell, you need to install the package libraries in the shell environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit\n",
    "!pip install opensearch-py\n",
    "!pip install opensearch_py_ml\n",
    "!pip install deprecated\n",
    "!pip install requests_aws4auth\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac226163",
   "metadata": {},
   "source": [
    "**Step 3:** Run the streamlit application from the shell. \n",
    "<br> The '--server.baseUrlPath' argument is used to set a custom base URL for the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e96561",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py --server.baseUrlPath=\"/proxy/absolute/8501\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52950b4",
   "metadata": {},
   "source": [
    "Access the app at this URL: https://semantic-search-nb-qnru.notebook.us-east-1.sagemaker.aws/proxy/absolute/8501/\n",
    "\n",
    "Update the above URL with the host name of your Jupyter notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
